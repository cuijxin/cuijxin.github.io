---
layout: post
title: Hadoop 实例 (一)
subtitle: 大数据
date: 2021-11-25
author:
header-img: img/Hadoop-hdfs-3370.png
catalog: true
tags:
  - 大数据
---

## 准备测试数据

测试数据包括两个 dept（部门）和 emp（员工），其中各字段用逗号分隔：

dept 文件内容：

```
10,ACCOUNTING,NEW YORK
20,RESEARCH,DALLAS
30,SALES,CHICAGO
40,OPERATIONS,BOSTON
```

emp 文件内容：

```
7369,SMITH,CLERK,7902,17-12月-80,800,,20
7499,ALLEN,SALESMAN,7698,20-2月-81,1600,300,30
7521,WARD,SALESMAN,7698,22-2月-81,1250,500,30
7566,JONES,MANAGER,7839,02-4月-81,2975,,20
7654,MARTIN,SALESMAN,7698,28-9月-81,1250,1400,30
7698,BLAKE,MANAGER,7839,01-5月-81,2850,,30
7782,CLARK,MANAGER,7839,09-6月-81,2450,,10
7839,KING,PRESIDENT,,17-11月-81,5000,,10
7844,TURNER,SALESMAN,7698,08-9月-81,1500,0,30
7900,JAMES,CLERK,7698,03-12月-81,950,,30
7902,FORD,ANALYST,7566,03-12月-81,3000,,20
7934,MILLER,CLERK,7782,23-1月-82,1300,,10
```

把这两个文件上传到 HDFS class6/input 目录中，执行如下命令：

```
cd /home/atguigu/mr/class6/input
hdfs dfs -mkdir -p /bigdata/class6/input
hdfs dfs -put dept /bigdata/class6/input
hdfs dfs -put emp /bigdata/class6/input
hdfs dfs -ls /bigdata/class6/input
```

## 测试例子 1：求各个部门的总工资

### 问题分析

MapReduce 中的 join 分为好几种，比如有最常见的 reduce side join、map side join 和 semi join 等。reduce join 在 shuffle 阶段要进行大量的数据传输，会造成大量的网络 IO 效率低下，而 map side
join 在处理多个小表关联大表时非常有用。

Map side join 是针对以下场景进行的优化：两个待连接表中，有一个非常大，而另一个非常小，以至于小表可以直接存放到内存中。这样我们可以将小表复制多份，让每个 map task 内存中存在一份（比如存放到 hash table 中），然后只扫描大表，对于大表中的每一条记录 key/value，在 hash table 中查找是否有相同的 key 的记录，如果有，则连接后输出即可。为了支持文件的复制，Hadoop 提供了一个类 DistributedCache，使用该类的方法如下：

1. 用户使用静态方法 DistributedCache.addCacheFile() 指定要复制的文件，它的参数是文件的 URI（如果是 HDFS 上的文件，可以这样：hdfs://jobtracker:50030/home/XXX/file）。JobTracker 在作业启动之前会获取这个 URI 列表，并将相应的文件拷贝到各个 TaskTracker 的本地磁盘上。

2. 用户使用 DistributedCache.getLocalCacheFiles() 方法获取文件目录，并使用标准的文件读写 API 读取相应的文件。

在下面代码中，将会把数据量小的表（部门 dept）缓存在内存中，在 Mapper 阶段对员工部门编号映射成部门名称，该名称作为 key 输出到 Reduce 中，在 Reduce 中按照部门计算各个部门的总工资。

### 处理流程图

![](/img/hadoop-q1-workflow.png)

### 测试代码

Q1SumDeptSalary.java 代码

```java
import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class Q1SumDeptSalary extends Configured implements Tool {

  public static class MapClass extends Mapper<LongWritable, Text, Text, Text> {

    // 用于缓存 dept文件中的数据
    private Map<String, String> deptMap = new HashMap<String, String>();
    private String[] kv;

    // 此方法会在Map方法执行之前执行且执行一次
    @Override
    protected void setup(Context context) throws IOException, InterruptedException {
      BufferedReader in = null;
      try {

        // 从当前作业中获取要缓存的文件
        Path[] paths = DistributedCache.getLocalCacheFiles(context.getConfiguration());
        String deptIdName = null;
        for (Path path : paths) {

          // 对部门文件字段进行拆分并缓存到deptMap中
          if (path.toString().contains("dept")) {
            in = new BufferedReader(new FileReader(path.toString()));
            while (null != (deptIdName = in.readLine())) {

              // 对部门文件字段进行拆分并缓存到deptMap中
              // 其中Map中key为部门编号，value为所在部门名称
              deptMap.put(deptIdName.split(",")[0], deptIdName.split(",")[1]);
            }
          }
        }
      } catch (IOException e) {
        e.printStackTrace();
      } finally {
        try {
          if (in != null) {
            in.close();
          }
        } catch (IOException e) {
          e.printStackTrace();
        }
      }
    }

    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

      // 对员工文件字段进行拆分
      kv = value.toString().split(",");

      // map join: 在map阶段过滤掉不需要的数据，输出key为部门名称和value为员工工资
      if (deptMap.containsKey(kv[7])) {
        if (null != kv[5] && !"".equals(kv[5].toString())) {
          context.write(new Text(deptMap.get(kv[7].trim())), new Text(kv[5].trim()));
        }
      }
    }
  }

  public static class Reduce extends Reducer<Text, Text, Text, LongWritable> {

    public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {

      // 对同一部门的员工工资进行求和
      long sumSalary = 0;
      for (Text val : values) {
        sumSalary += Long.parseLong(val.toString());
      }

      // 输出key为部门名称和value为该部门员工工资总和
      context.write(key, new LongWritable(sumSalary));
    }
  }

  @Override
  public int run(String[] args) throws Exception {

    // 实例化作业对象，设置作业名称、Mapper和Reduce类
    Job job = new Job(getConf(), "Q1SumDeptSalary");
    job.setJobName("Q1SumDeptSalary");
    job.setJarByClass(Q1SumDeptSalary.class);
    job.setMapperClass(MapClass.class);
    job.setReducerClass(Reduce.class);

    // 设置输入格式类
    job.setInputFormatClass(TextInputFormat.class);

    // 设置输出格式
    job.setOutputFormatClass(TextOutputFormat.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);

    // 第1个参数为缓存的部门数据路径、第2个参数为员工数据路径和第3个参数为输出路径
    String[] otherArgs = new GenericOptionsParser(job.getConfiguration(), args).getRemainingArgs();
    DistributedCache.addCacheFile(new Path(otherArgs[0]).toUri(), job.getConfiguration());
    FileInputFormat.addInputPath(job, new Path(otherArgs[1]));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));

    job.waitForCompletion(true);
    return job.isSuccessful() ? 0 : 1;
  }

  /**
   * 主方法，执行入口
   * @param args 输入参数
   */
  public static void main(String[] args) throws Exception {
    int res = ToolRunner.run(new Configuration(), new Q1SumDeptSalary(), args);
    System.exit(res);
  }
}
```

### 编译并打包代码

进入 /home/atguigu/mr/class6/Q1 目录，新建 Q1SumDeptSalary.java 代码文件，假设环境变量设置如下：

```
export JAVA_HOME=/opt/module/jdk1.8.0_144
export PATH=${JAVA_HOME}/bin:${PATH}
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
```

编译代码：

```
hadoop com.sun.tools.javac.Main Q1SumDeptSalary.java
jar cf Q1.jar Q1SumDeptSalary*.class
```

### 运行并查看结果

运行 Q1SumDeptSalary 时需要输入部门数据路径、员工数据路径和输出路径三个参数，需要注意的是 hdfs 的路径参数需要全路径，否则运行会报错：

- 部门数据路径：/bigdata/class6/input/dept，部门数据将缓存在各运行任务的节点内存中，可以提高处理效率。
- 员工数据路径：/bigdata/class6/input/emp。
- 输出路径：/bigdata/class6/out1

运行如下命令：

```
hadoop jar Q1.jar Q1SumDeptSalary /bigdata/class6/input/dept /bigdata/class6/input/emp /bigdata/class6/out1
```

运行成功后，刷新 HDFS 中的输出路径 /bigdata/class6/out1 目录，打开 part-r-00000 文件。

```
hdfs dfs -ls /bigdata/class6/out1
hdfs dfs -cat /bigdata/class6/out1/part-r-00000
```

可以看到运行结果：

```
ACCOUNTING 8750
RESEARCH 6775
SALES 9400
```

## 测试例子 2：求各个部门的人数和平均工资

### 问题分析

求各个部门的人数和平均工资，需要得到各部门工资总数和部门人数，通过两者相除获取各部门平均工资。首先和问题 1 类似在 Mapper 的 Setup 阶段缓存部门数据，然后在 Mapper 阶段抽取出部门编号和员工工资，利用缓存部门数据把部门编号对应为各部门名称，接着在 Shuffle 阶段把传过来的数据处理为部门名称对应该部门所有员工工资的列表，最后在 Reduce 中按照部门归组，遍历部门所有员工，求出总数和员工数，输出部门名称和平均工资。

### 处理流程图

![](/img/hadoop-q1-workflow.png)

### 编写代码

Q2DeptNumberAveSalary.java 代码：

```java
import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class Q2DeptNumberAveSalary extends Configured implements Tool {

  public static class MapClass extends Mapper<LongWritable, Text, Text, Text> {

    // 用于缓存 dept文件中的数据
    private Map<String, String> deptMap = new HashMap<String, String>();
    private String[] kv;

    // 此方法会在Map方法执行之前执行且执行一次
    @Override
    protected void setup(Context context) throws IOException, InterruptedException {
      BufferedReader in = null;
      try {
        // 从当前作业中获取要缓存的文件
        Path[] paths = DistributedCache.getLocalCacheFiles(context.getConfiguration());
        String deptIdName = null;
        for (Path path : paths) {

          // 对部门文件字段进行拆分并缓存到deptMap中
          if (path.toString().contains("dept")) {
            in = new BufferedReader(new FileReader(path.toString()));
            while (null != (deptIdName = in.readLine())) {

              // 对部门文件字段进行拆分并缓存到deptMap中
              // 其中Map中key为部门编号，value为所在部门名称
              deptMap.put(deptIdName.split(",")[0], deptIdName.split(",")[1]);
            }
          }
        }
      } catch (IOException e) {
        e.printStackTrace();
      } finally {
        try {
          if (in != null) {
            in.close();
          }
        } catch (IOException e) {
          e.printStackTrace();
        }
      }
    }

    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

      // 对员工文件字段进行拆分
      kv = value.toString().split(",");

      // map join: 在map阶段过滤掉不需要的数据，输出key为部门名称和value为员工工资
      if (deptMap.containsKey(kv[7])) {
        if (null != kv[5] && !"".equals(kv[5].toString())) {
          context.write(new Text(deptMap.get(kv[7].trim())), new Text(kv[5].trim()));
        }
      }
    }
  }

  public static class Reduce extends Reducer<Text, Text, Text, Text> {

    public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {

      long sumSalary = 0;
      int deptNumber = 0;

      // 对同一部门的员工工资进行求和
      for (Text val : values) {
        sumSalary += Long.parseLong(val.toString());
        deptNumber++;
      }

      // 输出key为部门名称和value为该部门员工工资平均值
      context.write(key, new Text("Dept Number:" + deptNumber + ", Ave Salary:" + sumSalary / deptNumber));
    }
  }

  @Override
  public int run(String[] args) throws Exception {

    // 实例化作业对象，设置作业名称、Mapper和Reduce类
    Job job = new Job(getConf(), "Q2DeptNumberAveSalary");
    job.setJobName("Q2DeptNumberAveSalary");
    job.setJarByClass(Q2DeptNumberAveSalary.class);
    job.setMapperClass(MapClass.class);
    job.setReducerClass(Reduce.class);

    // 设置输入格式类
    job.setInputFormatClass(TextInputFormat.class);

    // 设置输出格式类
    job.setOutputFormatClass(TextOutputFormat.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);

    // 第1个参数为缓存的部门数据路径、第2个参数为员工数据路径和第3个参数为输出路径
    String[] otherArgs = new GenericOptionsParser(job.getConfiguration(), args).getRemainingArgs();
    DistributedCache.addCacheFile(new Path(otherArgs[0]).toUri(), job.getConfiguration());
    FileInputFormat.addInputPath(job, new Path(otherArgs[1]));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[2]));

    job.waitForCompletion(true);
    return job.isSuccessful() ? 0 : 1;
  }

  /**
   * 主方法，执行入口
   * @param args 输入参数
   */
  public static void main(String[] args) throws Exception {
    int res = ToolRunner.run(new Configuration(), new Q2DeptNumberAveSalary(), args);
    System.exit(res);
  }
}
```

### 编译并打包代码

编译过程与 Q1 类似，这里不再赘述。最后获得 Q2.jar 包。

### 运行并查看结果

运行命令也与 Q1 类似，输入部门数据和员工数据的 HDFS 路径，以及输出路径。

```
hadoop jar Q2.jar Q2DeptNumberAveSalary hdfs://hadoop102:9000/bigdata/class6/input/dept hdfs://hadoop102:9000/bigdata/class6/input/emp hdfs://hadoop102:9000/bigdata/class6/out2
```

运行结束，刷新 HDFS 中的输出路径 /bigdata/class6/out2 目录，打开 part-r-00000 文件，可以查看到运行结果：

```
ACCOUNTING      Dept Number:3, Ave Salary:2916
RESEARCH        Dept Number:3, Ave Salary:2258
SALES   Dept Number:6, Ave Salary:1566
```
